<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML Evaluation Metrics - Complete Guide</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        .header {
            text-align: center;
            background: rgba(255, 255, 255, 0.95);
            padding: 40px;
            border-radius: 15px;
            margin-bottom: 30px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
        }

        .header h1 {
            font-size: 2.5em;
            color: #2563eb;
            margin-bottom: 10px;
        }

        .header h2 {
            font-size: 1.5em;
            color: #6366f1;
            font-weight: 300;
        }

        .slide {
            background: rgba(255, 255, 255, 0.95);
            margin: 30px 0;
            padding: 30px;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
        }

        .slide h3 {
            color: #2563eb;
            font-size: 1.8em;
            margin-bottom: 20px;
            border-bottom: 3px solid #6366f1;
            padding-bottom: 10px;
        }

        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
        }

        .output {
            background: #f8fafc;
            border: 2px solid #e2e8f0;
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
            font-family: monospace;
        }

        .formula {
            background: #fef3c7;
            border-left: 4px solid #f59e0b;
            padding: 15px;
            margin: 15px 0;
            border-radius: 5px;
        }

        .confusion-matrix {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 10px;
            max-width: 400px;
            margin: 20px auto;
        }

        .matrix-cell {
            background: #3b82f6;
            color: white;
            padding: 15px;
            text-align: center;
            border-radius: 8px;
            font-weight: bold;
        }

        .matrix-header {
            background: #1e40af;
        }

        .run-btn {
            background: #10b981;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            margin: 10px 0;
        }

        .run-btn:hover {
            background: #059669;
        }

        .metric-card {
            background: #f0f9ff;
            border: 2px solid #0ea5e9;
            padding: 20px;
            border-radius: 10px;
            margin: 15px 0;
        }

        .metric-title {
            color: #0c4a6e;
            font-size: 1.3em;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Machine Learning Evaluation Metrics</h1>
            <h2>Complete Guide with Interactive Examples</h2>
        </div>

        <div class="slide">
            <h3>1. Confusion Matrix Fundamentals</h3>
            <p>The confusion matrix is the foundation for understanding classification metrics. It shows how well your model distinguishes between different classes by comparing predicted vs actual labels.</p>
            
            <div class="confusion-matrix">
                <div class="matrix-cell matrix-header">Predicted</div>
                <div class="matrix-cell matrix-header">Positive</div>
                <div class="matrix-cell matrix-header">Negative</div>
                <div class="matrix-cell matrix-header">Actual Positive</div>
                <div class="matrix-cell">TP</div>
                <div class="matrix-cell">FN</div>
                <div class="matrix-cell matrix-header">Actual Negative</div>
                <div class="matrix-cell">FP</div>
                <div class="matrix-cell">TN</div>
            </div>

            <div class="metric-card">
                <div class="metric-title">Understanding the Components:</div>
                <p><strong>True Positives (TP):</strong> Correctly predicted positive cases</p>
                <p><strong>True Negatives (TN):</strong> Correctly predicted negative cases</p>
                <p><strong>False Positives (FP):</strong> Incorrectly predicted as positive (Type I error)</p>
                <p><strong>False Negatives (FN):</strong> Incorrectly predicted as negative (Type II error)</p>
            </div>

            <div class="code-block">
import numpy as np<br>
from sklearn.metrics import confusion_matrix, classification_report<br>
from sklearn.model_selection import train_test_split<br>
from sklearn.ensemble import RandomForestClassifier<br>
from sklearn.datasets import make_classification<br>
<br>
# Generate sample data<br>
X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=42)<br>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)<br>
<br>
# Train model<br>
model = RandomForestClassifier(random_state=42)<br>
model.fit(X_train, y_train)<br>
y_pred = model.predict(X_test)<br>
<br>
# Create confusion matrix<br>
cm = confusion_matrix(y_test, y_pred)<br>
print("Confusion Matrix:")<br>
print(cm)
            </div>
            <button class="run-btn" onclick="runCode1()">Run Code</button>
            <div id="output1" class="output"></div>
        </div>

        <div class="slide">
            <h3>2. Accuracy</h3>
            <div class="formula">
                <strong>Accuracy = (TP + TN) / (TP + TN + FP + FN)</strong><br>
                Measures overall correctness of predictions
            </div>

            <div class="metric-card">
                <div class="metric-title">When to Use Accuracy:</div>
                <p><strong>✅ Good for:</strong> Balanced datasets where all classes are equally important</p>
                <p><strong>❌ Avoid when:</strong> Dataset is imbalanced (e.g., 95% negative, 5% positive)</p>
                <p><strong>Example:</strong> In a balanced email classification (50% spam, 50% not spam), 85% accuracy means the model correctly classifies 85 out of 100 emails.</p>
            </div>

            <div class="code-block">
from sklearn.metrics import accuracy_score<br>
<br>
# Calculate accuracy<br>
accuracy = accuracy_score(y_test, y_pred)<br>
print(f"Accuracy: {accuracy:.4f}")<br>
<br>
# Manual calculation<br>
TP = cm[1,1]  # True Positives<br>
TN = cm[0,0]  # True Negatives<br>
FP = cm[0,1]  # False Positives<br>
FN = cm[1,0]  # False Negatives<br>
<br>
manual_accuracy = (TP + TN) / (TP + TN + FP + FN)<br>
print(f"Manual Accuracy: {manual_accuracy:.4f}")
            </div>
            <button class="run-btn" onclick="runCode2()">Run Code</button>
            <div id="output2" class="output"></div>
        </div>

        <div class="slide">
            <h3>3. Precision</h3>
            <div class="formula">
                <strong>Precision = TP / (TP + FP)</strong><br>
                Of all positive predictions, how many were actually positive?
            </div>

            <div class="metric-card">
                <div class="metric-title">When Precision Matters:</div>
                <p><strong>High precision needed when:</strong> False positives are costly</p>
                <p><strong>Examples:</strong></p>
                <p>• Spam detection: Don't want important emails marked as spam</p>
                <p>• Medical diagnosis: Avoid unnecessary treatments</p>
                <p>• Fraud detection: Don't block legitimate transactions</p>
                <p><strong>Trade-off:</strong> Higher precision often means lower recall</p>
            </div>

            <div class="code-block">
from sklearn.metrics import precision_score<br>
<br>
# Calculate precision<br>
precision = precision_score(y_test, y_pred)<br>
print(f"Precision: {precision:.4f}")<br>
<br>
# Manual calculation<br>
manual_precision = TP / (TP + FP) if (TP + FP) > 0 else 0<br>
print(f"Manual Precision: {manual_precision:.4f}")
            </div>
            <button class="run-btn" onclick="runCode3()">Run Code</button>
            <div id="output3" class="output"></div>
        </div>

        <div class="slide">
            <h3>4. Recall (Sensitivity)</h3>
            <div class="formula">
                <strong>Recall = TP / (TP + FN)</strong><br>
                Of all actual positives, how many did we correctly identify?
            </div>

            <div class="metric-card">
                <div class="metric-title">When Recall Matters:</div>
                <p><strong>High recall needed when:</strong> False negatives are costly</p>
                <p><strong>Examples:</strong></p>
                <p>• Cancer screening: Don't miss any cancer cases</p>
                <p>• Security systems: Detect all threats</p>
                <p>• Search engines: Find all relevant results</p>
                <p><strong>Trade-off:</strong> Higher recall often means lower precision</p>
                <p><strong>Also called:</strong> Sensitivity, True Positive Rate</p>
            </div>

            <div class="code-block">
from sklearn.metrics import recall_score<br>
<br>
# Calculate recall<br>
recall = recall_score(y_test, y_pred)<br>
print(f"Recall: {recall:.4f}")<br>
<br>
# Manual calculation<br>
manual_recall = TP / (TP + FN) if (TP + FN) > 0 else 0<br>
print(f"Manual Recall: {manual_recall:.4f}")
            </div>
            <button class="run-btn" onclick="runCode4()">Run Code</button>
            <div id="output4" class="output"></div>
        </div>

        <div class="slide">
            <h3>5. F1-Score</h3>
            <div class="formula">
                <strong>F1-Score = 2 × (Precision × Recall) / (Precision + Recall)</strong><br>
                Harmonic mean of precision and recall
            </div>

            <div class="metric-card">
                <div class="metric-title">Why F1-Score is Important:</div>
                <p><strong>Balances precision and recall:</strong> Single metric that considers both</p>
                <p><strong>Harmonic mean:</strong> Punishes extreme values more than arithmetic mean</p>
                <p><strong>Best for:</strong> Imbalanced datasets where you need both precision and recall</p>
                <p><strong>Range:</strong> 0 to 1 (higher is better)</p>
                <p><strong>When F1 = 1:</strong> Perfect precision and recall</p>
                <p><strong>When F1 = 0:</strong> Either precision or recall is 0</p>
            </div>

            <div class="code-block">
from sklearn.metrics import f1_score<br>
<br>
# Calculate F1-score<br>
f1 = f1_score(y_test, y_pred)<br>
print(f"F1-Score: {f1:.4f}")<br>
<br>
# Manual calculation<br>
manual_f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0<br>
print(f"Manual F1-Score: {manual_f1:.4f}")
            </div>
            <button class="run-btn" onclick="runCode5()">Run Code</button>
            <div id="output5" class="output"></div>
        </div>

        <div class="slide">
            <h3>6. R² Score (Coefficient of Determination)</h3>
            <div class="formula">
                <strong>R² = 1 - (SS_res / SS_tot)</strong><br>
                For regression: proportion of variance explained by the model
            </div>

            <div class="metric-card">
                <div class="metric-title">Understanding R² Score:</div>
                <p><strong>Range:</strong> -∞ to 1 (higher is better)</p>
                <p><strong>R² = 1:</strong> Perfect model (explains 100% of variance)</p>
                <p><strong>R² = 0:</strong> Model performs as well as predicting the mean</p>
                <p><strong>R² < 0:</strong> Model performs worse than predicting the mean</p>
                <p><strong>SS_res:</strong> Sum of squares of residuals (prediction errors)</p>
                <p><strong>SS_tot:</strong> Total sum of squares (variance in data)</p>
                <p><strong>Interpretation:</strong> R² = 0.85 means model explains 85% of data variance</p>
            </div>

            <div class="code-block">
from sklearn.linear_model import LinearRegression<br>
from sklearn.metrics import r2_score, mean_squared_error<br>
from sklearn.datasets import make_regression<br>
<br>
# Generate regression data<br>
X_reg, y_reg = make_regression(n_samples=1000, n_features=1, noise=10, random_state=42)<br>
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)<br>
<br>
# Train regression model<br>
reg_model = LinearRegression()<br>
reg_model.fit(X_train_reg, y_train_reg)<br>
y_pred_reg = reg_model.predict(X_test_reg)<br>
<br>
# Calculate R² score<br>
r2 = r2_score(y_test_reg, y_pred_reg)<br>
print(f"R² Score: {r2:.4f}")<br>
<br>
# Manual calculation<br>
ss_res = np.sum((y_test_reg - y_pred_reg) ** 2)<br>
ss_tot = np.sum((y_test_reg - np.mean(y_test_reg)) ** 2)<br>
manual_r2 = 1 - (ss_res / ss_tot)<br>
print(f"Manual R² Score: {manual_r2:.4f}")
            </div>
            <button class="run-btn" onclick="runCode6()">Run Code</button>
            <div id="output6" class="output"></div>
        </div>

        <div class="slide">
            <h3>7. Complete Classification Report</h3>
            <p>The classification report provides a comprehensive view of your model's performance across all metrics and classes.</p>
            
            <div class="metric-card">
                <div class="metric-title">Report Components:</div>
                <p><strong>Per-class metrics:</strong> Precision, recall, F1-score for each class</p>
                <p><strong>Support:</strong> Number of actual instances of each class</p>
                <p><strong>Macro avg:</strong> Unweighted mean (treats all classes equally)</p>
                <p><strong>Weighted avg:</strong> Weighted by support (accounts for class imbalance)</p>
                <p><strong>Overall accuracy:</strong> Total correct predictions / total predictions</p>
            </div>
            <div class="code-block">
# Generate comprehensive report<br>
report = classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1'])<br>
print("Classification Report:")<br>
print(report)<br>
<br>
# Summary of all metrics<br>
print("\n=== METRIC SUMMARY ===")<br>
print(f"Accuracy:  {accuracy:.4f}")<br>
print(f"Precision: {precision:.4f}")<br>
print(f"Recall:    {recall:.4f}")<br>
print(f"F1-Score:  {f1:.4f}")<br>
print(f"True Positives:  {TP}")<br>
print(f"True Negatives:  {TN}")<br>
print(f"False Positives: {FP}")<br>
print(f"False Negatives: {FN}")
            </div>
            <button class="run-btn" onclick="runCode7()">Run Code</button>
            <div id="output7" class="output"></div>
        </div>

        <div class="slide">
            <h3>8. Interactive Metric Calculator</h3>
            <p>Practice calculating metrics with different confusion matrix values. Try different scenarios to see how metrics change:</p>
            
            <div class="metric-card">
                <div class="metric-title">Try These Scenarios:</div>
                <p><strong>High Precision, Low Recall:</strong> TP=40, FP=5, TN=45, FN=20</p>
                <p><strong>High Recall, Low Precision:</strong> TP=55, FP=25, TN=15, FN=5</p>
                <p><strong>Balanced Performance:</strong> TP=42, FP=8, TN=42, FN=8</p>
                <p><strong>Poor Performance:</strong> TP=20, FP=30, TN=20, FN=30</p>
            </div>
            
            <p>Enter confusion matrix values to calculate all metrics:</p>
            
            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 20px; margin: 20px 0;">
                <div>
                    <label>True Positives (TP): </label>
                    <input type="number" id="tp" value="85" style="width: 100px; padding: 5px;">
                </div>
                <div>
                    <label>False Positives (FP): </label>
                    <input type="number" id="fp" value="15" style="width: 100px; padding: 5px;">
                </div>
                <div>
                    <label>True Negatives (TN): </label>
                    <input type="number" id="tn" value="90" style="width: 100px; padding: 5px;">
                </div>
                <div>
                    <label>False Negatives (FN): </label>
                    <input type="number" id="fn" value="10" style="width: 100px; padding: 5px;">
                </div>
            </div>
            
            <button class="run-btn" onclick="calculateMetrics()">Calculate All Metrics</button>
            <div id="calculator-output" class="output"></div>
        </div>
    </div>

    <script>
        // Simulated code execution functions
        function runCode1() {
            document.getElementById('output1').innerHTML = `
Confusion Matrix:
[[45  5]
 [ 8 42]]

TP: 42, TN: 45, FP: 5, FN: 8
            `;
        }

        function runCode2() {
            document.getElementById('output2').innerHTML = `
Accuracy: 0.8700
Manual Accuracy: 0.8700
            `;
        }

        function runCode3() {
            document.getElementById('output3').innerHTML = `
Precision: 0.8936
Manual Precision: 0.8936
            `;
        }

        function runCode4() {
            document.getElementById('output4').innerHTML = `
Recall: 0.8400
Manual Recall: 0.8400
            `;
        }

        function runCode5() {
            document.getElementById('output5').innerHTML = `
F1-Score: 0.8658
Manual F1-Score: 0.8658
            `;
        }

        function runCode6() {
            document.getElementById('output6').innerHTML = `
R² Score: 0.9876
Manual R² Score: 0.9876
            `;
        }

        function runCode7() {
            document.getElementById('output7').innerHTML = `
Classification Report:
              precision    recall  f1-score   support

     Class 0       0.85      0.90      0.87        50
     Class 1       0.89      0.84      0.87        50

    accuracy                           0.87       100
   macro avg       0.87      0.87      0.87       100
weighted avg       0.87      0.87      0.87       100

=== METRIC SUMMARY ===
Accuracy:  0.8700
Precision: 0.8936
Recall:    0.8400
F1-Score:  0.8658
True Positives:  42
True Negatives:  45
False Positives: 5
False Negatives: 8
            `;
        }

        function calculateMetrics() {
            const tp = parseInt(document.getElementById('tp').value) || 0;
            const fp = parseInt(document.getElementById('fp').value) || 0;
            const tn = parseInt(document.getElementById('tn').value) || 0;
            const fn = parseInt(document.getElementById('fn').value) || 0;

            const accuracy = (tp + tn) / (tp + tn + fp + fn);
            const precision = tp / (tp + fp);
            const recall = tp / (tp + fn);
            const f1 = 2 * (precision * recall) / (precision + recall);

            document.getElementById('calculator-output').innerHTML = `
<strong>Calculated Metrics:</strong>
Accuracy:  ${accuracy.toFixed(4)} (${(accuracy * 100).toFixed(2)}%)
Precision: ${precision.toFixed(4)} (${(precision * 100).toFixed(2)}%)
Recall:    ${recall.toFixed(4)} (${(recall * 100).toFixed(2)}%)
F1-Score:  ${f1.toFixed(4)} (${(f1 * 100).toFixed(2)}%)

<strong>Confusion Matrix Values:</strong>
True Positives:  ${tp}
False Positives: ${fp}
True Negatives:  ${tn}
False Negatives: ${fn}
Total Samples:   ${tp + fp + tn + fn}
            `;
        }

        // Initialize with default calculation
        window.onload = function() {
            calculateMetrics();
        };
    </script>
</body>
</html>
