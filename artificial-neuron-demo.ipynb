{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Artificial Neurons & Activation Functions\n",
    "\n",
    "Interactive demonstration of neural network building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU Function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Sigmoid Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "# Softmax Function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Visualize Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# ReLU\n",
    "axes[0].plot(x, relu(x), 'r-', linewidth=3, label='ReLU')\n",
    "axes[0].set_title('ReLU Function', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Sigmoid\n",
    "axes[1].plot(x, sigmoid(x), 'b-', linewidth=3, label='Sigmoid')\n",
    "axes[1].set_title('Sigmoid Function', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "# Comparison\n",
    "axes[2].plot(x, relu(x), 'r-', linewidth=2, label='ReLU')\n",
    "axes[2].plot(x, sigmoid(x), 'b-', linewidth=2, label='Sigmoid')\n",
    "axes[2].plot(x, np.tanh(x), 'g-', linewidth=2, label='Tanh')\n",
    "axes[2].set_title('Comparison', fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Softmax Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example logits for 3 classes\n",
    "logits = np.array([[2.0, 1.0, 0.1],\n",
    "                   [1.0, 3.0, 0.2],\n",
    "                   [0.1, 0.2, 3.0]])\n",
    "\n",
    "# Apply softmax\n",
    "probabilities = np.array([softmax(logit) for logit in logits])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Raw logits\n",
    "x_pos = np.arange(len(logits))\n",
    "width = 0.25\n",
    "\n",
    "for i in range(3):\n",
    "    ax1.bar(x_pos + i*width, logits[:, i], width, \n",
    "            label=f'Class {i+1}', alpha=0.8)\n",
    "\n",
    "ax1.set_title('Raw Logits', fontweight='bold')\n",
    "ax1.set_xlabel('Sample')\n",
    "ax1.set_ylabel('Value')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Softmax probabilities\n",
    "for i in range(3):\n",
    "    ax2.bar(x_pos + i*width, probabilities[:, i], width, \n",
    "            label=f'Class {i+1}', alpha=0.8)\n",
    "\n",
    "ax2.set_title('Softmax Probabilities', fontweight='bold')\n",
    "ax2.set_xlabel('Sample')\n",
    "ax2.set_ylabel('Probability')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Probability sums:\")\n",
    "for i, probs in enumerate(probabilities):\n",
    "    print(f\"Sample {i+1}: {probs.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Artificial Neuron Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialNeuron:\n",
    "    def __init__(self, n_inputs, activation='relu'):\n",
    "        self.weights = np.random.randn(n_inputs) * 0.1\n",
    "        self.bias = 0.0\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        z = np.dot(self.weights, inputs) + self.bias\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            return relu(z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return sigmoid(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Neuron Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neurons with different activations\n",
    "neurons = {\n",
    "    'ReLU': ArtificialNeuron(3, activation='relu'),\n",
    "    'Sigmoid': ArtificialNeuron(3, activation='sigmoid')\n",
    "}\n",
    "\n",
    "# Test inputs\n",
    "test_inputs = np.array([\n",
    "    [1.0, -0.5, 2.0],\n",
    "    [-1.0, 0.5, -2.0],\n",
    "    [0.0, 0.0, 0.0],\n",
    "    [2.0, 2.0, 2.0]\n",
    "])\n",
    "\n",
    "print(\"Neuron Outputs:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, inputs in enumerate(test_inputs):\n",
    "    print(f\"\\nInput {i+1}: {inputs}\")\n",
    "    for name, neuron in neurons.items():\n",
    "        output = neuron.forward(inputs)\n",
    "        print(f\"  {name:8}: {output:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Interactive Activation Function Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plot with different parameters\n",
    "def plot_activation_with_params():\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    # Different ReLU variants\n",
    "    ax.plot(x, relu(x), 'r-', linewidth=3, label='ReLU')\n",
    "    ax.plot(x, np.maximum(0.1*x, x), 'm-', linewidth=3, label='Leaky ReLU (Î±=0.1)')\n",
    "    ax.plot(x, sigmoid(x), 'b-', linewidth=3, label='Sigmoid')\n",
    "    ax.plot(x, np.tanh(x), 'g-', linewidth=3, label='Tanh')\n",
    "    \n",
    "    ax.set_title('Activation Function Comparison', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Input (x)')\n",
    "    ax.set_ylabel('Output f(x)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_activation_with_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "- **ReLU**: Simple, efficient, good for hidden layers\n",
    "- **Sigmoid**: Outputs probabilities, good for binary classification\n",
    "- **Softmax**: Perfect for multi-class classification\n",
    "- **Choose wisely**: Different functions for different purposes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
